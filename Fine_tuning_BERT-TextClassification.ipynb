{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine tuning BERT ",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMShC1rgNShZE0GwNSGYW2f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7afca853442949dd87b73b7f2afdc949": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8e6b8b9b7a1541ce9c38946df20c6d7d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dd5d5902be214ecd8328800a9543d9db",
              "IPY_MODEL_247de68f881e46d58d65229282487b81"
            ]
          }
        },
        "8e6b8b9b7a1541ce9c38946df20c6d7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dd5d5902be214ecd8328800a9543d9db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_12d8eb4428cd489c95c2a5d79b761af2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a3496368a3294dcd83426e3c2dfe8b36"
          }
        },
        "247de68f881e46d58d65229282487b81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_365aa9f7ef944085b5d252571d069f39",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:45&lt;00:00, 5.12kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f02659d88c4f4b25a75423d8f3950388"
          }
        },
        "12d8eb4428cd489c95c2a5d79b761af2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a3496368a3294dcd83426e3c2dfe8b36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "365aa9f7ef944085b5d252571d069f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f02659d88c4f4b25a75423d8f3950388": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "006edc0cf91f49d78708e7cbf67f3b38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e587869d3660478499cbd6b120976b26",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ac5d3906ef4f4a69bcf5dd31494bbe0f",
              "IPY_MODEL_13a7b7f5efec448d8634d812173a9ba1"
            ]
          }
        },
        "e587869d3660478499cbd6b120976b26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ac5d3906ef4f4a69bcf5dd31494bbe0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5aef99aa30ca418b903abc2a88733e3e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_16668d71b60f499c866045cfc856f276"
          }
        },
        "13a7b7f5efec448d8634d812173a9ba1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_759a4d467a6249d38c95175214678b9c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 1.38kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3e3ba94a7a4b4ee2a7bdec9ca44aa536"
          }
        },
        "5aef99aa30ca418b903abc2a88733e3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "16668d71b60f499c866045cfc856f276": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "759a4d467a6249d38c95175214678b9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3e3ba94a7a4b4ee2a7bdec9ca44aa536": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "783470dfb5ea452f99da99dedcd24dc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6ca22fbd47854d649618493b0aa9cd54",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_13b7a33d89e04b2fb05ae8e7f943342e",
              "IPY_MODEL_c07c3d235251416db29e5647a420585c"
            ]
          }
        },
        "6ca22fbd47854d649618493b0aa9cd54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "13b7a33d89e04b2fb05ae8e7f943342e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_198f6c966bf04d1fa5ae98129c780eec",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1fa0d1d1db134e74baa51eb13337b09e"
          }
        },
        "c07c3d235251416db29e5647a420585c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d4e0cc39f938431d9c59532c6674d7d1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:06&lt;00:00, 65.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ddb49bcd007b40f0ba865b4d6b3041a4"
          }
        },
        "198f6c966bf04d1fa5ae98129c780eec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1fa0d1d1db134e74baa51eb13337b09e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d4e0cc39f938431d9c59532c6674d7d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ddb49bcd007b40f0ba865b4d6b3041a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhishek-P/bert-trials/blob/main/Fine_tuning_BERT-TextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1rvvVuN7sBv"
      },
      "source": [
        "# Fine tuning BERT\n",
        "Based on the tutorial from http://mccormickml.com/2019/07/22/BERT-fine-tuning/#11-using-colab-gpu-for-training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTiWuOMb7rno",
        "outputId": "36647a3f-12cf-4156-96c9-5f41f082b2a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vXzIzkP8xHq",
        "outputId": "79b5d71e-ce12-4e22-a739-a192ccd44359",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  print(\"There are %d GPUs available\" % torch.cuda.device_count())\n",
        "  print(' We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "  print(' No GPU available, using CPU instead')\n",
        "  device = torch.device('cpu')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPUs available\n",
            " We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRp4nihs75d-",
        "outputId": "f6ae5054-3d99-40ee-edfc-946fa232b883",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "is_colab = False\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  print('Running on CoLab')\n",
        "  is_colab = True\n",
        "else:\n",
        "  print('Not running on CoLab')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on CoLab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e03tvGaa8Q2t",
        "outputId": "012402f5-67bc-4f61-85a3-8614739c5ddd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "if is_colab:\n",
        "    %pwd\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive', force_remount=True)\n",
        "    print(\"Mounted Drive successfully\")\n",
        "    %cd /content\n",
        "    %cd /content/gdrive/My\\ Drive/abhip-models/\n",
        "    print(\"Should be in the drive folder of the project, check previous printed line to confirm\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Mounted Drive successfully\n",
            "/content\n",
            "/content/gdrive/My Drive/abhip-models\n",
            "Should be in the drive folder of the project, check previous printed line to confirm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUPjsO4I9Z0U",
        "outputId": "dbaf1b9e-5ae9-4b54-b95f-20370b8ac072",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/f4/9f93f06dd2c57c7cd7aa515ffbf9fcfd8a084b92285732289f4a5696dd91/transformers-3.2.0-py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 38.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 40.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 37.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=8ceecf61299e0ce926fcb9d8b383173e2c79f101d698b009699578a998bb02e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0v8KAig7_h-"
      },
      "source": [
        "I don't expect to have consistent runtime for the work and would be useful to keep them in peristent storage in My Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOp2sjKu8jbZ",
        "outputId": "1654e743-20ce-4642-b299-2b5f2a3567b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/My Drive/abhip-models'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuuF_IRO9ufx",
        "outputId": "967edcef-a960-435b-c493-891bbadc9e85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=85a77cc5ef7b8c3e1e2335e3f519d4ad0d3d7c4209467804807eb3384a4c47d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3qNAKmTAhVl"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrJwekHB-RlN",
        "outputId": "2c665854-cb8c-4fcf-8a23-4cfbaae49b85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading the dataset')\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "  try: \n",
        "    wget.download(url, './cola_public_1.1.zip')\n",
        "  except Exception as e:\n",
        "    print(\"Failed to use py wget\")\n",
        "    print(e)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading the dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnaWOxUkAOkT"
      },
      "source": [
        "if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFkIxML9AWne",
        "outputId": "9c49c266-769e-4643-afd6-4b2e1e9e955d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\",  delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "df.sample(10)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 8,551\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7154</th>\n",
              "      <td>sks13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>This girl will not buy bread, will she buy bread?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2419</th>\n",
              "      <td>l-93</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>The new tax Jaws will gain the middle class.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6488</th>\n",
              "      <td>d_98</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>John liked everything that was placed before him.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4455</th>\n",
              "      <td>ks08</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>I found myself doing need sleep.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4061</th>\n",
              "      <td>ks08</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>We hope the availability of such a vaccine in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5808</th>\n",
              "      <td>c_13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>George wrote a volume of poems in Latin for Jane.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2377</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I admired him for his honesty.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>cj99</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Mary gets depressed and she listens to the Gra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2467</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>In this cave was found an ancient treasure trove.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2405</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I incorporated the new results into the paper.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "7154           sks13  ...  This girl will not buy bread, will she buy bread?\n",
              "2419            l-93  ...       The new tax Jaws will gain the middle class.\n",
              "6488            d_98  ...  John liked everything that was placed before him.\n",
              "4455            ks08  ...                   I found myself doing need sleep.\n",
              "4061            ks08  ...  We hope the availability of such a vaccine in ...\n",
              "5808            c_13  ...  George wrote a volume of poems in Latin for Jane.\n",
              "2377            l-93  ...                     I admired him for his honesty.\n",
              "163             cj99  ...  Mary gets depressed and she listens to the Gra...\n",
              "2467            l-93  ...  In this cave was found an ancient treasure trove.\n",
              "2405            l-93  ...     I incorporated the new results into the paper.\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-xjzuC4Bdsl",
        "outputId": "ad92dd40-91e5-40a2-95f1-c5d98e49e57a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2485</th>\n",
              "      <td>The cook boned the fish of its backbone.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2530</th>\n",
              "      <td>Jennifer craned her arm.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7892</th>\n",
              "      <td>Bill's reading Shakespeare and Maureen's singi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3188</th>\n",
              "      <td>She always clad herself in black.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5791</th>\n",
              "      <td>the book with a red cover from Blackwell by Ro...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "2485           The cook boned the fish of its backbone.      0\n",
              "2530                           Jennifer craned her arm.      0\n",
              "7892  Bill's reading Shakespeare and Maureen's singi...      0\n",
              "3188                  She always clad herself in black.      0\n",
              "5791  the book with a red cover from Blackwell by Ro...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AfWONfrDFnx"
      },
      "source": [
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXuQ19fnDgal",
        "outputId": "826b387a-c840-43c3-e3f9-783d3597a3b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "7afca853442949dd87b73b7f2afdc949",
            "8e6b8b9b7a1541ce9c38946df20c6d7d",
            "dd5d5902be214ecd8328800a9543d9db",
            "247de68f881e46d58d65229282487b81",
            "12d8eb4428cd489c95c2a5d79b761af2",
            "a3496368a3294dcd83426e3c2dfe8b36",
            "365aa9f7ef944085b5d252571d069f39",
            "f02659d88c4f4b25a75423d8f3950388"
          ]
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7afca853442949dd87b73b7f2afdc949",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAeDqxgEGbl5",
        "outputId": "223c3029-bbaa-4ee6-9654-23e33ea428bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print('Original: ', sentences[0])\n",
        "\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En5aVK4zGkAE",
        "outputId": "f1f261bf-48b4-46a0-c354-57fedfec3992",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_len = 0\n",
        "for sent in sentences:\n",
        "  input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "  max_len = max(max_len, len(input_ids))\n",
        "print(\"Max length:\", max_len)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length: 47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZvWQ0-0VPqr",
        "outputId": "6a67986b-062b-49c3-e7a4-9884f4a893c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if  (not os.path.exists('./input_ids.pt')) or (not os.path.exists('./attention_masks.pt')) or (not os.path.exists('./labels.pt')):\n",
        "  input_ids =[]\n",
        "  attention_masks = []\n",
        "\n",
        "  for sent in sentences:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        sent,\n",
        "        add_special_tokens = True, \n",
        "        max_length = 64,\n",
        "        pad_to_max_length = True,\n",
        "        return_attention_mask = True, \n",
        "        return_tensors = 'pt'\n",
        "    )\n",
        "\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.tensor(labels)\n",
        "\n",
        "  torch.save(input_ids, 'input_ids.pt')\n",
        "  torch.save(attention_masks, 'attention_masks.pt')\n",
        "  torch.save(labels, 'labels.pt')\n",
        "\n",
        "\n",
        "  print('Original:  ', sentences[0])\n",
        "  print('Token IDs: ', input_ids[0])\n",
        "else:\n",
        "  print(\"Loading the tensors from Drive\")\n",
        "  input_ids = torch.load('./input_ids.pt')\n",
        "  attention_masks = torch.load('./attention_masks.pt')\n",
        "  labels = torch.load('./labels.pt')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading the tensors from Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXbW74RLn-nn",
        "outputId": "bbbadbe3-e408-4c60-dc88-f655b627268e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7,695 training samples\n",
            "  856 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fabLvCBpe2tr"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, \n",
        "    sampler = RandomSampler(train_dataset),\n",
        "    batch_size = batch_size\n",
        ")\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    sampler = SequentialSampler(val_dataset),\n",
        "    batch_size = batch_size\n",
        ")\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-i0BtU-bTjb_",
        "outputId": "a2a26d18-b6c5-4608-fbe6-015165b6fbee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "006edc0cf91f49d78708e7cbf67f3b38",
            "e587869d3660478499cbd6b120976b26",
            "ac5d3906ef4f4a69bcf5dd31494bbe0f",
            "13a7b7f5efec448d8634d812173a9ba1",
            "5aef99aa30ca418b903abc2a88733e3e",
            "16668d71b60f499c866045cfc856f276",
            "759a4d467a6249d38c95175214678b9c",
            "3e3ba94a7a4b4ee2a7bdec9ca44aa536",
            "783470dfb5ea452f99da99dedcd24dc9",
            "6ca22fbd47854d649618493b0aa9cd54",
            "13b7a33d89e04b2fb05ae8e7f943342e",
            "c07c3d235251416db29e5647a420585c",
            "198f6c966bf04d1fa5ae98129c780eec",
            "1fa0d1d1db134e74baa51eb13337b09e",
            "d4e0cc39f938431d9c59532c6674d7d1",
            "ddb49bcd007b40f0ba865b4d6b3041a4"
          ]
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels = 2,\n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False\n",
        ")\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "006edc0cf91f49d78708e7cbf67f3b38",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "783470dfb5ea452f99da99dedcd24dc9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxnaoxqiYH6w",
        "outputId": "978e252d-476d-4c77-f487-abea5f21fcfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        }
      },
      "source": [
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wigIXRwfgVr"
      },
      "source": [
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr  = 2e-5,\n",
        "    eps =  1e-8\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkmIY0uefu7B"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "epochs = 4\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs;\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                           num_warmup_steps =0,\n",
        "                                           num_training_steps = total_steps)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwl_7bdAgN3f"
      },
      "source": [
        "import numpy as np\n",
        "def flat_accuracy(preds, labels):\n",
        "  pred_flat = np.argmax(preds, axis = 1).flatten()\n",
        "  labels_flat = labels.flatten()\n",
        "  return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkTroCJLipvx"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "  elapsed_rounded = int(round(elapsed))\n",
        "  return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFy2iAAcjC4T",
        "outputId": "7474db3c-aa75-4019-eba6-1d0ac649db47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Training Loop\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "\n",
        "training_stats = []\n",
        "\n",
        "total_t0 = time.time()\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "  print(\"\")\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch_i +1, epochs))\n",
        "  print('Training...')\n",
        "\n",
        "\n",
        "  t0 = time.time()\n",
        "\n",
        "  total_train_loss = 0\n",
        "  # Put it into training mode\n",
        "  model.train()\n",
        "\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    if step%40 == 0 and not step == 0:\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "\n",
        "      print('Batch {:>5}  of  {:>5}.   Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "      b_input_ids = batch[0].to(device)\n",
        "      b_input_mask = batch[1].to(device)\n",
        "      b_labels = batch[2].to(device)\n",
        "\n",
        "      model.zero_grad()\n",
        "\n",
        "      loss, logits = model(b_input_ids,\n",
        "                           token_type_ids = None,\n",
        "                           attention_mask = b_input_mask, \n",
        "                           labels = b_labels)\n",
        "\n",
        "      total_train_loss += loss.item()\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      scheduler.step()\n",
        "\n",
        "  avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "  training_time = format_time(time.time() - t0)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\" Average training loss {0:.2f}\".format(avg_train_loss))\n",
        "  print(\" Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Running Validation...\")\n",
        "\n",
        "\n",
        "  t0 = time.time()\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  total_eval_accuracy = 0\n",
        "  total_eval_loss = 0\n",
        "  nb_eval_steps = 0\n",
        "\n",
        "  for batch in validation_dataloader:\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      (loss, logits) = model(b_input_ids,\n",
        "                             token_type_ids = None, \n",
        "                             attention_mask = b_input_mask,\n",
        "                             labels = b_labels)\n",
        "      \n",
        "    total_eval_loss =+ loss.item()\n",
        "\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "  \n",
        "\n",
        "  avg_val_accuracy = total_eval_accuracy/ len(validation_dataloader)\n",
        "  print(\" Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "  avg_val_loss = total_eval_loss/ len(validation_dataloader)\n",
        "  validation_time = format_time(time.time() - t0)\n",
        "  print(\" Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "  print(\" Validation took: {:}\".format(validation_time))\n",
        "\n",
        "  training_stats.append(\n",
        "      {\n",
        "          'epoch': epoch_i + 1,\n",
        "       'Training Loss': avg_train_loss,\n",
        "       'Valid. Loss': avg_val_loss,\n",
        "       'Valid. Accur.': avg_val_accuracy,\n",
        "       'Training Time': training_time,\n",
        "       'Validation Time': validation_time\n",
        "      }\n",
        "  )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0)))\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "Batch    40  of    241.   Elapsed: 0:00:00.\n",
            "Batch    80  of    241.   Elapsed: 0:00:01.\n",
            "Batch   120  of    241.   Elapsed: 0:00:01.\n",
            "Batch   160  of    241.   Elapsed: 0:00:01.\n",
            "Batch   200  of    241.   Elapsed: 0:00:02.\n",
            "Batch   240  of    241.   Elapsed: 0:00:02.\n",
            "\n",
            " Average training loss 0.02\n",
            " Training epoch took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            " Accuracy: 0.71\n",
            " Validation Loss: 0.02\n",
            " Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "Batch    40  of    241.   Elapsed: 0:00:00.\n",
            "Batch    80  of    241.   Elapsed: 0:00:00.\n",
            "Batch   120  of    241.   Elapsed: 0:00:01.\n",
            "Batch   160  of    241.   Elapsed: 0:00:01.\n",
            "Batch   200  of    241.   Elapsed: 0:00:01.\n",
            "Batch   240  of    241.   Elapsed: 0:00:02.\n",
            "\n",
            " Average training loss 0.02\n",
            " Training epoch took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            " Accuracy: 0.71\n",
            " Validation Loss: 0.02\n",
            " Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "Batch    40  of    241.   Elapsed: 0:00:00.\n",
            "Batch    80  of    241.   Elapsed: 0:00:00.\n",
            "Batch   120  of    241.   Elapsed: 0:00:01.\n",
            "Batch   160  of    241.   Elapsed: 0:00:01.\n",
            "Batch   200  of    241.   Elapsed: 0:00:01.\n",
            "Batch   240  of    241.   Elapsed: 0:00:02.\n",
            "\n",
            " Average training loss 0.02\n",
            " Training epoch took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            " Accuracy: 0.71\n",
            " Validation Loss: 0.02\n",
            " Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "Batch    40  of    241.   Elapsed: 0:00:00.\n",
            "Batch    80  of    241.   Elapsed: 0:00:00.\n",
            "Batch   120  of    241.   Elapsed: 0:00:01.\n",
            "Batch   160  of    241.   Elapsed: 0:00:01.\n",
            "Batch   200  of    241.   Elapsed: 0:00:01.\n",
            "Batch   240  of    241.   Elapsed: 0:00:02.\n",
            "\n",
            " Average training loss 0.02\n",
            " Training epoch took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            " Accuracy: 0.71\n",
            " Validation Loss: 0.02\n",
            " Validation took: 0:00:03\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:21 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulLzVUGFu4aL",
        "outputId": "d0078e88-78c0-452c-f2a8-297dcbe21794",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "df_stats"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0:00:02</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0:00:02</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0:00:02</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0:00:02</td>\n",
              "      <td>0:00:03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               0.02         0.02           0.71       0:00:02         0:00:03\n",
              "2               0.02         0.02           0.71       0:00:02         0:00:03\n",
              "3               0.02         0.02           0.71       0:00:02         0:00:03\n",
              "4               0.02         0.02           0.71       0:00:02         0:00:03"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEwxMZYA05tW",
        "outputId": "690acf84-a30b-409c-c8d9-13175d5c311b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "sns.set(font_scale = 1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1,2,3,4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuRK9YrL7Kg0",
        "outputId": "0ffb54d4-849b-4c9e-aba7-3a9a3607ce49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter = \"\\t\", header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "\n",
        "for sent in sentences:\n",
        "  encoded_dict = tokenizer.encode_plus(\n",
        "      sent, \n",
        "      add_special_tokens = True,\n",
        "      max_length = 64, \n",
        "      pad_to_max_length = True,\n",
        "      return_attention_mask = True,\n",
        "      return_tensors = 'pt'\n",
        "      )\n",
        "  \n",
        "  input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "  attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim = 0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size = batch_size)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 516\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI_eZRX29JnK",
        "outputId": "8c7133ca-279a-4a45-8e40-98c8d1b849fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "model.eval()\n",
        "\n",
        "predictions, true_labels =[], []\n",
        "\n",
        "for batch in prediction_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(b_input_ids, token_type_ids= None, attention_mask = b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "\n",
        "print('DONE.')\n",
        "print(len(predictions))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 516 test sentences...\n",
            "DONE.\n",
            "17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnNaGscE-3D-",
        "outputId": "7405c494-721b-442f-a106-ae4683d0c0da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum())/ len(df.label) * 100))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples: 354 of 516 (68.60%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WThxAjyL_beC",
        "outputId": "509a9b6c-24f2-403c-a183-5897b30719c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "print('Calculating Mattews Corr. Coef. for each batch...')\n",
        "\n",
        "for i in range(len(true_labels)):\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n",
        "  matthews_set.append(matthews)\n",
        "\n",
        "print(matthews_set)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Mattews Corr. Coef. for each batch...\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdMxyA6-Arv4",
        "outputId": "20f6deea-6be3-47ef-cb12-93a9e8e8f958",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "ax = sns.barplot(x = list(range(len(matthews_set))), y = matthews_set, ci = )\n",
        "\n",
        "plt.title(' MCC Score per batch')\n",
        "plt.ylabel('MCC Score (-1 to +1)')\n",
        "plt.xlabel('Batch #')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-ec206f0eb548>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ax = sns.barplot(x = list(range(len(matthews_set))), y = matthews_set, ci = )\u001b[0m\n\u001b[0m                                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0PSqUO6yNKG"
      },
      "source": [
        " \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('Total MCC: %.3f' % mcc)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}